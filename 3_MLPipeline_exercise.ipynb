{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "exercise-title",
      "metadata": {},
      "source": [
        "# In-Class Exercise: Preventing Data Leakage & Building Robust ML Pipelines\n",
        "\n",
        "In this exercise, you will apply what you have learned about data splitting, avoiding data leakage, using pipelines for preprocessing and modeling, and saving/loading your models. Follow each task and complete the required code in the provided cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise-instructions",
      "metadata": {},
      "source": [
        "## Instructions\n",
        "\n",
        "1. **Import Libraries & Simulate Data:**\n",
        "   - Import the required libraries.\n",
        "   - Simulate 90 days of ice cream sales data. (A template is provided below.)\n",
        "\n",
        "2. **Task 1 – Data Splitting and Standardization:**\n",
        "   - Split the data into training and testing sets.\n",
        "   - Apply standard scaling to the training data using `fit_transform()` and transform the test data using `transform()`.\n",
        "   - *Hint:* Use the `StandardScaler` from scikit-learn.\n",
        "\n",
        "3. **Task 2 – Building a Pipeline:**\n",
        "   - Construct a ML pipeline that scales the `temperature` feature and passes the `promotion` feature unchanged.\n",
        "   - Append a RandomForestRegressor to your pipeline.\n",
        "\n",
        "4. **Task 3 – Demonstrating Data Leakage:**\n",
        "   - Create a version of your dataset that erroneously includes the target variable as a feature.\n",
        "   - Train a model on this leaked dataset and observe the difference in performance.\n",
        "\n",
        "5. **Task 4 – Model Persistence:**\n",
        "   - Save your trained pipeline to disk using `pickle`.\n",
        "   - Load the saved model and validate it using the test data.\n",
        "\n",
        "6. **Reflection:**\n",
        "   - In your own words, describe why it is important to prevent data leakage and how pipelines help in maintaining reproducible workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "import-libraries",
      "metadata": {},
      "source": [
        "### Step 1: Import Libraries & Simulate Data\n",
        "\n",
        "Use the following template code to import libraries and simulate ice cream sales data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-import-simulate",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pickle\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# Simulate 90 days of ice cream sales data\n",
        "np.random.seed(42)\n",
        "n_days = 90\n",
        "start_date = datetime(2024, 1, 1)\n",
        "dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
        "\n",
        "temperatures = np.random.normal(loc=25, scale=3, size=n_days).round(1)\n",
        "promotions = np.random.choice([0, 1], size=n_days, p=[0.7, 0.3])\n",
        "\n",
        "sales = 300 + 12 * temperatures + 60 * promotions + np.random.normal(0, 20, size=n_days)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'temperature': temperatures,\n",
        "    'promotion': promotions,\n",
        "    'sales': sales.round().astype(int)\n",
        "})\n",
        "\n",
        "print(df.head())\n",
        "print(\"Data simulation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task1-header",
      "metadata": {},
      "source": [
        "## Task 1 – Data Splitting and Standardization\n",
        "\n",
        "Split the simulated data into training and testing sets. Then, apply `StandardScaler` to the training set using `fit_transform()` and to the test set using `transform()`. Complete the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Split the data into features (X) and target (y).\n",
        "X = df[['temperature', 'promotion']]\n",
        "y = df['sales']\n",
        "\n",
        "# TODO: Split the data into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Initialize the StandardScaler and apply fit_transform on X_train and transform on X_test for the 'temperature' feature only.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply scaling on the 'temperature' column of the training set\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled['temperature'] = scaler.fit_transform(X_train[['temperature']])\n",
        "\n",
        "# Apply the same transformation on the test set\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled['temperature'] = scaler.transform(X_test[['temperature']])\n",
        "\n",
        "print(\"Data splitting and scaling complete.\")\n",
        "print(X_train_scaled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task2-header",
      "metadata": {},
      "source": [
        "## Task 2 – Building a Pipeline\n",
        "\n",
        "Build a machine learning pipeline that:\n",
        "\n",
        "- Preprocesses the data by scaling the `temperature` feature while leaving the `promotion` feature unchanged.\n",
        "- Trains a `RandomForestRegressor` on the preprocessed data.\n",
        "\n",
        "Complete the code in the cell below. Hint: use `ColumnTransformer` and `Pipeline` from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task2-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# TODO: Create a ColumnTransformer to scale only the 'temperature' feature\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scale_temp', StandardScaler(), ['temperature'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# TODO: Construct a Pipeline with the preprocessor and a RandomForestRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# TODO: Train the pipeline using the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(\"Pipeline training complete.\")\n",
        "print(\"Pipeline training score:\", pipeline.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task3-header",
      "metadata": {},
      "source": [
        "## Task 3 – Demonstrating Data Leakage\n",
        "\n",
        "Create a version of your dataset that erroneously includes the target variable (`sales`) as a feature. Train a model on this leaked dataset and compare the performance with your previous correct approach. Complete the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a copy of the dataset and include 'sales' in the features to simulate leakage\n",
        "X_leak = df[['temperature', 'promotion']].copy()\n",
        "X_leak['leak_feature'] = df['sales']  # Incorrect addition of target variable as a feature\n",
        "y_leak = df['sales']\n",
        "\n",
        "# Split the leaked dataset\n",
        "X_train_leak, X_test_leak, y_train_leak, y_test_leak = train_test_split(X_leak, y_leak, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestRegressor on the leaked data\n",
        "leak_model = RandomForestRegressor(random_state=42)\n",
        "leak_model.fit(X_train_leak, y_train_leak)\n",
        "\n",
        "train_score_leak = leak_model.score(X_train_leak, y_train_leak)\n",
        "test_score_leak = leak_model.score(X_test_leak, y_test_leak)\n",
        "\n",
        "print(f\"Training R^2 with leakage: {train_score_leak:.4f}\")\n",
        "print(f\"Testing R^2 with leakage: {test_score_leak:.4f}\")\n",
        "print(\"Note the unusually high testing score that indicates data leakage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "task4-header",
      "metadata": {},
      "source": [
        "## Task 4 – Model Persistence\n",
        "\n",
        "Save your trained pipeline (from Task 2) using `pickle` and then load it back. Validate the loaded model by checking its performance on the test set. Complete the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task4-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Save the trained pipeline to disk using pickle\n",
        "model_filename = \"student_model_pipeline.pkl\"\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(pipeline, file)\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "# TODO: Load the saved model and validate it on the test data\n",
        "with open(model_filename, 'rb') as file:\n",
        "    loaded_pipeline = pickle.load(file)\n",
        "\n",
        "loaded_score = loaded_pipeline.score(X_test, y_test)\n",
        "print(f\"Loaded pipeline test score: {loaded_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reflection",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "In a few sentences, describe why it is important to prevent data leakage in model training and how the use of pipelines ensures a more reproducible and robust workflow.\n",
        "\n",
        "*Write your answers in the cell below (as markdown or comments in a code cell).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reflection-answer",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your reflection here\n",
        "# For example:\n",
        "# \"Preventing data leakage is crucial because it ensures that the model only learns\n",
        "# from the training data and does not have access to any information from the test set.\n",
        "# This leads to more realistic performance estimates and prevents overfitting. \n",
        "# Pipelines help encapsulate preprocessing and modeling steps in a unified framework,\n",
        "# reducing the chance of accidental leakage and ensuring reproducibility across experiments.\"\n",
        "\n",
        "# (Feel free to modify the text above with your own reflections.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}