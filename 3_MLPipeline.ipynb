{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53f71c6a-3d56-4e99-8d8e-68d4489e7f79",
      "metadata": {},
      "source": [
        "# Lecture Notes: Preventing Data Leakage and Building Robust Machine Learning Pipelines\n",
        "\n",
        "In these notes, we cover how to simulate and visualize data, understand the dangers of data leakage, and build robust machine learning pipelines. Our example will focus on forecasting ice cream sales. The following topics are covered:\n",
        "\n",
        "**Table of Contents**\n",
        "1. Introduction: Ice Cream Sales Forecasting\n",
        "2. Setting Up: Importing Required Libraries\n",
        "3. Data Simulation: Creating Sales Data\n",
        "4. Data Visualization: Spotting Trends\n",
        "5. A Naïve Approach: Using the Entire Dataset\n",
        "6. Demonstrating Data Leakage\n",
        "7. A Careful Approach: Proper Train-Test Split and Standardization\n",
        "8. Detailed Illustration: fit() vs. transform()\n",
        "9. Leveraging Pipelines to Prevent Data Leakage\n",
        "10. Model Persistence with pickle\n",
        "11. Extended Concepts: Train, Validation, and Test Datasets & The ML Pipeline\n",
        "12. Summary and Best Practices\n",
        "\n",
        "<a name=\"introduction\"></a>\n",
        "## 1. Introduction: Ice Cream Sales Forecasting\n",
        "\n",
        "In this lecture, we explore the steps required to forecast ice cream sales while avoiding data leakage. We cover how to simulate data, split and preprocess it, and how to build a robust pipeline that ensures every step—from data preparation to evaluation—is properly executed. Finally, we explain the importance of saving your trained model for future use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89c3fd1-823e-4afd-9b76-0dc6d02b16cf",
      "metadata": {},
      "source": [
        "<a name=\"setting-up\"></a>\n",
        "## 2. Setting Up: Importing Required Libraries\n",
        "\n",
        "Before starting our analysis, we first import all the necessary Python libraries. These include libraries for numerical computations, data manipulation, visualization, and machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9718b60d-3b12-4c60-bc6d-3b32e0312961",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import pickle  # For model serialization\n",
        "\n",
        "print(\"All libraries have been successfully imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcb7ecd-fcfd-4cae-95d0-c9c412bd2415",
      "metadata": {},
      "source": [
        "<a name=\"data-simulation\"></a>\n",
        "## 3. Data Simulation: Creating Sales Data\n",
        "\n",
        "We simulate 90 days of ice cream sales data. The features include:\n",
        "\n",
        "- **Temperature:** Simulated using a normal distribution (mean = 25°C, standard deviation = 3).\n",
        "- **Promotion:** A binary variable indicating whether a promotion is active (1) or not (0), with a 30% chance of promotion.\n",
        "\n",
        "Sales are computed using the formula:\n",
        "\n",
        "  sales = 300 + 12 × temperature + 60 × promotion + random noise (mean = 0, std = 20)\n",
        "\n",
        "This formula is designed to mimic a scenario where higher temperatures and promotions increase sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2fef33-65e7-4e0b-a9c5-913d74b013ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# 1. Generate dates (90 days)\n",
        "n_days = 90\n",
        "start_date = datetime(2024, 1, 1)\n",
        "dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
        "\n",
        "# 2. Generate temperature and promotion features\n",
        "temperatures = np.random.normal(loc=25, scale=3, size=n_days).round(1)\n",
        "promotions = np.random.choice([0, 1], size=n_days, p=[0.7, 0.3])\n",
        "\n",
        "# 3. Compute sales using the given formula\n",
        "sales = 300 + 12 * temperatures + 60 * promotions + np.random.normal(0, 20, size=n_days)\n",
        "\n",
        "# 4. Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'temperature': temperatures,\n",
        "    'promotion': promotions,\n",
        "    'sales': sales.round().astype(int)\n",
        "})\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\n(Data simulation successful.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde643ae-2c25-4638-ba58-0a6918e2d2ad",
      "metadata": {},
      "source": [
        "<a name=\"data-visualization\"></a>\n",
        "## 4. Data Visualization: Spotting Trends\n",
        "\n",
        "Visualizing the data helps us understand patterns. We will create two visualizations:\n",
        "\n",
        "1. **Scatter Plot (Temperature vs Sales):** This plot shows how temperature and promotions influence sales.\n",
        "2. **Boxplot (Promotion vs Sales):** This plot compares sales on days with and without promotions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237e9d86-176a-408d-8d9d-5c78b2d85919",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(df['temperature'], df['sales'], c=df['promotion'], cmap='coolwarm', alpha=0.7)\n",
        "plt.title('Temperature vs Sales (Color by Promotion)')\n",
        "plt.xlabel('Temperature (°C)')\n",
        "plt.ylabel('Sales')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Promotion', rotation=270, labelpad=15)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49b0977-3deb-4d74-82bd-167ebc464d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "groups = df.groupby('promotion')['sales']\n",
        "labels = ['No Promo (0)', 'Promo (1)']\n",
        "data_list = [groups.get_group(g) for g in sorted(groups.groups.keys())]\n",
        "plt.boxplot(data_list, labels=labels)\n",
        "plt.title('Promotion vs Sales - Boxplot')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The boxplot shows higher sales during promotions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f2af4b-345f-4baf-a675-c88a9de2138b",
      "metadata": {},
      "source": [
        "<a name=\"naive-approach\"></a>\n",
        "## 5. A Naïve Approach: Using the Entire Dataset\n",
        "\n",
        "A naïve approach is to use the entire dataset for both training and evaluation. Although the model may appear to perform well (high R² score), this approach can conceal issues such as overfitting and does not reflect the model's true generalization capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f39667-3ab9-47ae-87d5-635c5c6f2be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Naïve Approach: Using the entire dataset for training and evaluation ===\")\n",
        "\n",
        "X_all = df[['temperature', 'promotion']]\n",
        "y_all = df['sales']\n",
        "\n",
        "model_all = RandomForestRegressor(random_state=42)\n",
        "model_all.fit(X_all, y_all)\n",
        "\n",
        "r2_all = model_all.score(X_all, y_all)\n",
        "print(f\"Model R^2 using the entire dataset: {r2_all:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c6e158-5d8b-4725-9b00-eca514371c6f",
      "metadata": {},
      "source": [
        "<a name=\"demonstrating-leakage\"></a>\n",
        "## 6. Demonstrating Data Leakage\n",
        "\n",
        "Data leakage occurs when information from outside the training dataset is used to create the model. Here, a common mistake is to include the target variable as a feature. This example demonstrates how leakage can lead to unrealistically high performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b90f8c07-50a1-48b0-bc34-53a94af18ec2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Demonstrating Data Leakage ===\")\n",
        "\n",
        "X_leak = df[['temperature', 'promotion']].copy()\n",
        "X_leak['target_leak'] = df['sales']  # Incorrectly adding the target as a feature\n",
        "y_leak = df['sales']\n",
        "\n",
        "X_train_leak, X_test_leak, y_train_leak, y_test_leak = train_test_split(\n",
        "    X_leak, y_leak, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "leak_model = RandomForestRegressor(random_state=42)\n",
        "leak_model.fit(X_train_leak, y_train_leak)\n",
        "\n",
        "train_score_leak = leak_model.score(X_train_leak, y_train_leak)\n",
        "test_score_leak = leak_model.score(X_test_leak, y_test_leak)\n",
        "\n",
        "print(f\"Training R^2 with leakage: {train_score_leak:.4f}\")\n",
        "print(f\"Testing R^2 with leakage: {test_score_leak:.4f}\")\n",
        "print(\"Notice how the test score is suspiciously high—this indicates leakage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5c63f9-451a-4a91-a69c-976806f6d171",
      "metadata": {},
      "source": [
        "<a name=\"careful-approach\"></a>\n",
        "## 7. A Careful Approach: Proper Train-Test Split and Standardization\n",
        "\n",
        "A better strategy involves:\n",
        "\n",
        "- Splitting the data into training and test sets.\n",
        "- Fitting a scaler on the training data.\n",
        "- Applying the same transformation to the test data.\n",
        "\n",
        "This approach prevents data leakage by ensuring that the test data remains unseen during model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e2b990-23aa-4667-a662-70d5e7f8cfce",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Careful Approach: Manual Data Splitting and Standardization ===\")\n",
        "\n",
        "X = df[['temperature', 'promotion']]\n",
        "y = df['sales']\n",
        "\n",
        "# 1. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Scale the training data using fit_transform()\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# 3. Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "r2_train = model.score(X_train_scaled, y_train)\n",
        "r2_test = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Training R^2: {r2_train:.4f}\")\n",
        "print(f\"Testing R^2: {r2_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d1e0f8-ec6f-4ebc-a549-fa1a6f0f7543",
      "metadata": {},
      "source": [
        "<a name=\"fit-vs-transform\"></a>\n",
        "## 8. Detailed Illustration: fit() vs. transform()\n",
        "\n",
        "When preprocessing, **fit()** calculates necessary parameters (e.g., mean and standard deviation) from the training data. **transform()** then applies these parameters to new data. It is critical to:\n",
        "\n",
        "- Use `fit_transform()` only on the training data.\n",
        "- Use `transform()` on the test data.\n",
        "\n",
        "Below is an illustration using just the temperature feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5ca7d8-a3bd-42df-a754-6d3e40d8c305",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using only the temperature feature for demonstration\n",
        "X_ice = df[['temperature']]\n",
        "y_ice = df['sales']\n",
        "\n",
        "X_train_ice, X_test_ice, y_train_ice, y_test_ice = train_test_split(\n",
        "    X_ice, y_ice, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train_ice.shape, \"Test set size:\", X_test_ice.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e084b6ab-bcfd-43eb-8d2c-edea71a86ce8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correct: fit on training set and then transform test set\n",
        "scaler_correct = StandardScaler()\n",
        "X_train_ice_scaled_correct = scaler_correct.fit_transform(X_train_ice)\n",
        "X_test_ice_scaled_correct = scaler_correct.transform(X_test_ice)\n",
        "\n",
        "print(\"Transformed training data (first 5 rows):\")\n",
        "print(X_train_ice_scaled_correct[:5])\n",
        "print(\"\\nTransformed test data (first 5 rows):\")\n",
        "print(X_test_ice_scaled_correct[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6213ab7-0a3a-4ec0-9a89-125d0d1a4f86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Incorrect: Using fit_transform() on the test data causes leakage\n",
        "scaler_wrong = StandardScaler()\n",
        "X_test_ice_scaled_wrong = scaler_wrong.fit_transform(X_test_ice)\n",
        "\n",
        "print(\"\\n(Incorrect) Transformed test data using fit_transform (first 5 rows):\")\n",
        "print(X_test_ice_scaled_wrong[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338f2d2b-4a0a-46a9-abf7-95b3f0bf1c02",
      "metadata": {},
      "source": [
        "<a name=\"pipelines\"></a>\n",
        "## 9. Leveraging Pipelines to Prevent Data Leakage\n",
        "\n",
        "Pipelines combine multiple preprocessing steps with model training so that every process occurs in a fixed, leak-free order. In our example, we build a pipeline that scales selected features and then trains a Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe3a38b-0131-4e56-933b-99474f0bd3b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "print(\"=== Using a Pipeline to Combine Preprocessing and Modeling ===\")\n",
        "\n",
        "# Split the data\n",
        "X_pipeline = df[['temperature', 'promotion']]\n",
        "y_pipeline = df['sales']\n",
        "\n",
        "X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(\n",
        "    X_pipeline, y_pipeline, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create a preprocessor that scales 'temperature' and leaves 'promotion' unchanged\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('scale_temp', StandardScaler(), ['temperature'])\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Build the pipeline: preprocessing followed by model training\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Fit the pipeline (fit_transform is applied on the training data internally)\n",
        "pipeline.fit(X_train_pipe, y_train_pipe)\n",
        "\n",
        "r2_train_pipe = pipeline.score(X_train_pipe, y_train_pipe)\n",
        "r2_test_pipe = pipeline.score(X_test_pipe, y_test_pipe)\n",
        "\n",
        "print(f\"Pipeline Training R^2: {r2_train_pipe:.4f}\")\n",
        "print(f\"Pipeline Testing R^2: {r2_test_pipe:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170b1b7c-c5b4-439b-8d42-db73e59060f2",
      "metadata": {},
      "source": [
        "<a name=\"model-persistence\"></a>\n",
        "## 10. Model Persistence with pickle: Saving the Best Model\n",
        "\n",
        "After training a robust model using a pipeline, it is important to save it for future use. We use `pickle` to serialize the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947b3b8e-3f25-41ac-82cf-c8d033bc0bc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Saving and Loading the Pipeline using pickle ===\")\n",
        "\n",
        "model_filename = \"model_pipeline.pkl\"\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "print(f\"Model has been saved to {model_filename}\")\n",
        "\n",
        "# To load the model later:\n",
        "with open(model_filename, 'rb') as f:\n",
        "    loaded_pipeline = pickle.load(f)\n",
        "print(f\"Pipeline loaded from {model_filename} and ready to use.\")\n",
        "\n",
        "# Validate the loaded model by scoring on the test data\n",
        "loaded_test_score = loaded_pipeline.score(X_test_pipe, y_test_pipe)\n",
        "print(f\"Loaded Pipeline Testing R^2: {loaded_test_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba634c22-2c63-4a5b-a45d-e01e7a4d7e15",
      "metadata": {},
      "source": [
        "<a name=\"extended-concepts\"></a>\n",
        "## 11. Extended Concepts: Train, Validation, and Test Datasets & The ML Pipeline\n",
        "\n",
        "In more advanced settings, data is often divided into three sets:\n",
        "\n",
        "- **Training Set:** Used for model fitting.\n",
        "- **Validation Set:** Used to fine-tune the model (e.g., hyperparameter tuning).\n",
        "- **Test Set:** Used for a final unbiased evaluation of the model.\n",
        "\n",
        "A comprehensive machine learning pipeline involves:\n",
        "\n",
        "1. **Data Preparation and Cleaning:** Select and clean your features.\n",
        "2. **Feature Engineering:** Create and select relevant features.\n",
        "3. **Data Splitting:** Divide data into training, validation, and test sets.\n",
        "4. **Model Selection and Training:** Experiment with and tune various models.\n",
        "5. **Evaluation:** Rigorously assess model performance.\n",
        "6. **Deployment and Monitoring:** Deploy the model and monitor its performance over time.\n",
        "\n",
        "Additionally, environments like Anaconda allow easy package and environment management, helping maintain reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63966e8-7968-4f2b-8275-0aee67d6e8de",
      "metadata": {},
      "source": [
        "<a name=\"summary\"></a>\n",
        "## 12. Summary and Best Practices\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "- **Prevent Data Leakage:** Never allow test data to influence model training. Always ensure that preprocessing is done separately for training and test sets.\n",
        "- **Data Splitting:** Properly divide your dataset into training, validation, and test sets for a more accurate evaluation.\n",
        "- **Use Pipelines:** Encapsulate preprocessing and modeling steps into a pipeline to maintain a consistent and leak-free process.\n",
        "- **Model Persistence:** Save your trained model using tools like pickle to ensure reproducibility and future deployment.\n",
        "- **Development Tools:** Leverage environments and tools (such as Anaconda) for easy management of packages and dependencies.\n",
        "\n",
        "By adhering to these best practices, you can build robust machine learning models that generalize well and perform reliably in real-world applications.\n",
        "\n",
        "Happy modeling!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
