{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85a8d86",
   "metadata": {},
   "source": [
    "Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) for `DecisionTreeRegressor`; check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for ``RandomForestRegressor``; check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) for `GradientBoostingRegressor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df636d",
   "metadata": {},
   "source": [
    "# 1. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da64fbc",
   "metadata": {},
   "source": [
    "The dataset ``ram_price.csv`` contains the price information of some historical computer memory: Random Access Memory (RAM). \n",
    "\n",
    "- As the relationship between ``price`` and ``date`` is quite skewed (you may visualize them with a simple scatterplot), we take log transformation of ``price`` for better fit with the linear regression model.\n",
    "\n",
    "- For tree model, it doesn't matter whether we take log transformation of ``price`` or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cabda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram = pd.read_csv(\"ram_price.csv\")\n",
    "\n",
    "ram['log_price'] = np.log(ram['price'])    # Add a new column named log_price \n",
    "\n",
    "display(ram.shape, ram.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1479d6",
   "metadata": {},
   "source": [
    "**Split the Data into Train/Test**\n",
    "\n",
    "Arbitrarily split the data based on ``date`` so that we use historical data (``date < 2000``) to forecast RAM prices after the year 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ram[ram['date'] < 2000]       \n",
    "data_test = ram[ram['date'] >= 2000]\n",
    "\n",
    "X_train = data_train[['date']]       # Features should in 2D\n",
    "y_train = data_train['log_price']\n",
    "\n",
    "X_test = data_test[['date']]\n",
    "y_test = data_test['log_price']\n",
    "\n",
    "display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfb8f3",
   "metadata": {},
   "source": [
    "**Visualization**\n",
    "\n",
    "Here we visualize train and test data with different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeddd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_train['date'], data_train['log_price'], linewidth = 3, label=\"Train data\")                 # Train\n",
    "plt.plot(data_test['date'], data_test['log_price'], color='lightblue', linewidth = 3, label=\"Test data\") # Test\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel(\"(Log) Price/Mbyte\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf726ef5",
   "metadata": {},
   "source": [
    "**Model Training & Evaluation**\n",
    "\n",
    "Now, let's train ``Linear Regression`` and ``Decision Tree Regressor``, with ``log_price`` as the target variable. Check the train & test ``R2`` accordingly.\n",
    "\n",
    "- The ``Regression Tree`` model overfit very much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# For reproducible result, fix random_state in tree models\n",
    "tr = DecisionTreeRegressor(max_depth=3, random_state = 1).fit(X_train, y_train)  \n",
    "\n",
    "print(\"Linear Regression Train & Test R2: {:.2f}, {:.2f}\".format(lr.score(X_train,y_train),lr.score(X_test,y_test)))\n",
    "print(\"Regression Tree Train & Test R2: {:.2f}, {:.2f}\".format(tr.score(X_train,y_train),tr.score(X_test,y_test)))  # negative test R2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f47d34",
   "metadata": {},
   "source": [
    "**Visualize the Tree model**\n",
    "\n",
    "- Note the ``impurity`` of each node (as indicated by color) is measured by ``squared_error (i.e., MSE)`` in regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab156d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(figsize=(18,10))  \n",
    "\n",
    "plot_tree(decision_tree = tr, \n",
    "          feature_names = X_train.columns,    # Or X_train.columns.to_list() if using an old version of scikit-learn   \n",
    "          filled = True,   \n",
    "          fontsize = 10)  \n",
    "\n",
    "fig.suptitle('Regession Tree for RAM Log_Price', fontsize = 15);   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9bc7d3",
   "metadata": {},
   "source": [
    "In the root node, (1) what is the predicted price for instances falling into this node? (2) what is the measurement of impurity in this node? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad039bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. Predicted price in the root node: \", y_train.mean())\n",
    "print(\"2. MSE measures the impurity: \", sum((y_train - y_train.mean())**2)/len(y_train) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e035c3d",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 1: Your Codes Here***</font>  \n",
    "\n",
    "\n",
    "**Visualize the Actual and Predicted log_price (the model)**\n",
    "\n",
    "- To display the predicted ``log_price`` for train and test set, you may need to get all features ``ram[['date']]`` for prediction first.\n",
    "- Visualize the predicted values with a dashline. You may use format string ``\"--\"`` within `plt.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e93b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all data (both train and test)\n",
    "X_all = ram[['date']]\n",
    "pred_tr = tr.predict(X_all) \n",
    "pred_lr = lr.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ccd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual log_price on y axis  \n",
    "# Train\n",
    "# Test\n",
    "\n",
    "# Plot the predicted log_price on y axis  \n",
    "# Predicted by tree\n",
    "# Predicted by lr\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel(\"(Log) Price/Mbyte\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e053d19",
   "metadata": {},
   "source": [
    "# 2. Ensemble of Trees  \n",
    "\n",
    "\n",
    "Let's use the ``boston_house_prices.csv`` data again.  As data scaling doesn't matter for tree models, let's skip the preprocessing steps here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9da474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('boston_house_prices.csv')\n",
    "\n",
    "# Separate data into train and test\n",
    "X = df.drop(columns = 'MEDV')\n",
    "y = df['MEDV']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)    \n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae660d18",
   "metadata": {},
   "source": [
    "## 2.1 Build Three Models Individualy \n",
    "\n",
    "Let's compare 3 tree models: ``DecisionTreeRegressor``, ``RandomForestRegressor`` and ``GradientBoostingRegressor``, all with default settings.  \n",
    "\n",
    "- You will see ``DecisionTreeRegressor`` has overfited, and ``GradientBoostingRegressor`` returns the best performance on test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a61d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = DecisionTreeRegressor(random_state = 0)       # max_depth = None, min_samples_leaf = 1 (default)   \n",
    "tr.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 0)       # n_estimators = 100, max_depth = None, min_samples_leaf=1 (default)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state = 0)   # n_estimators = 100, learning_rate=0.1, max_depth = 3 (default)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"tr Train vs Test R2: {:.2f} vs {:.2f}\".format(tr.score(X_train, y_train), tr.score(X_test, y_test)))  \n",
    "print(\"rf Train vs. Test R2: {:.2f} vs. {:.2f}\".format(rf.score(X_train, y_train), rf.score(X_test, y_test)))\n",
    "print(\"gb Train vs Test R2: {:.2f} vs. {:.2f}\".format(gb.score(X_train, y_train), gb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d835f43",
   "metadata": {},
   "source": [
    "Checkout all the trees we've trained! You can even use ``plot_tree()`` function to visualize them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03385e25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rf.estimators_   # A list with 100 estimators, use rf.estimators_[0] to select the first tree\n",
    "#gb.estimators_   # A 2D array with 100 estimators (100,1), use gb.estimators_[0,0] to select the first tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474517e2",
   "metadata": {},
   "source": [
    "## 2.2  Model and Parameter Comparison with GridSearchCV\n",
    "\n",
    "Let's use ``GridSearchCV`` to compare the three models and their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ec1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline with only one step \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('regressor', DecisionTreeRegressor())])    # Put any model in this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce720de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the param_grid using a list of dictionaries\n",
    "\n",
    "# tune max_depth, min_samples_leaf, max_features for tree (3 * 4 *3 = 36)\n",
    "param1 = {'regressor': [DecisionTreeRegressor()],\n",
    "          'regressor__max_depth':[3,5,7],\n",
    "          'regressor__min_samples_leaf':[10,20,30,40],\n",
    "          'regressor__max_features':[0.5, 0.8, 1]}        # Compare %features randomly selected for spliting\n",
    "\n",
    "# tune n_estimators, max_samples, max_features, max_depth for random forest (3 * 4 * 3 * 3 = 108)\n",
    "param2 = {'regressor':[RandomForestRegressor()],\n",
    "          'regressor__max_depth':[3,5,7], \n",
    "          'regressor__n_estimators':[50,100,200],           \n",
    "          'regressor__max_samples':[0.1, 0.5, 0.8, 1],     # %bootstraped sample/training data size\n",
    "          'regressor__max_features':[0.5, 0.8, 1]}         # Compare %features randomly selected for splitting\n",
    "\n",
    "# tune n_estimators, learning_rate, subsample, max_feature for gradient boosting (3 * 3 * 4 * 3 = 108) \n",
    "param3 = {'regressor':[GradientBoostingRegressor()],\n",
    "          'regressor__n_estimators':[50,100,200],           \n",
    "          'regressor__learning_rate':[0.1, 1, 10],         # Learning rate \n",
    "          'regressor__subsample':[0.1, 0.5, 0.8, 1],       # %data randomly sampled for each stage\n",
    "          'regressor__max_features':[0.5,0.8,1]}           # Compare %features randomly selected for splitting\n",
    "\n",
    "params = [param1, param2, param3]    # Wrap multiple dictionaries in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3b0e7",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 2: Your Codes Here***</font>  \n",
    "\n",
    "With the above parameter grid, please fit the GridSearchCV object on the train set and answer below questions:\n",
    "\n",
    "- What is the best model and associated parameter? \n",
    "- That is the cross-validation score for the best model/paramters?\n",
    "- How many models have been built in order to find the best model and params?( *Hints: check the `params` key*)\n",
    "- Check the generalization performance of the best model refit on the training data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5a84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object on train set, with 5-fold cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7ed63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No. of models trained in cross validation (GridSearch)\n",
    "\n",
    "len(grid.cv_results_['params']) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model's performance on test set\n",
    "\n",
    "grid.score(X_test, y_test)     \n",
    "\n",
    "# Alternatively, uncomment the below \n",
    "#best = grid.best_estimator_\n",
    "#best.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
