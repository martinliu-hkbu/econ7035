{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "grouped-header",
      "metadata": {
        "id": "grouped-header"
      },
      "source": [
        "# Lecture Notes: Preventing Data Leakage and Building Robust Machine Learning Pipelines\n",
        "\n",
        "In these notes, we cover how to simulate and visualize data, understand the dangers of data leakage, and build robust machine learning pipelines. Our example focuses on forecasting ice cream sales.\n",
        "\n",
        "## Table of Contents\n",
        "1. **Introduction: Ice Cream Sales Forecasting**\n",
        "2. **An Incorrect Approach:**\n",
        "3. **Train, Validation, and Test Datasets**\n",
        "4. **The ML Pipeline**\n",
        "5. **A Better Approach:**\n",
        "6. **Summary and Best Practices**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section1-header",
      "metadata": {
        "id": "section1-header"
      },
      "source": [
        "## 1. Introduction: Ice Cream Sales Forecasting\n",
        "\n",
        "In this lecture, we explore the steps required to forecast ice cream sales while avoiding data leakage. We cover how to simulate data, split and preprocess it, and how to build a robust pipeline that ensures every step—from data preparation to evaluation—is properly executed. Finally, we explain the importance of saving your trained model for future use.\n",
        "\n",
        "*(The following cells also include setting up our environment, simulating sales data, and visualizing it.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section1-setup",
      "metadata": {
        "id": "section1-setup"
      },
      "source": [
        "<a name=\"setting-up\"></a>\n",
        "### Setting Up: Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-setup",
      "metadata": {
        "id": "code-setup"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import pickle  # For model serialization\n",
        "\n",
        "print(\"All libraries have been successfully imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section1-data-simulation",
      "metadata": {
        "id": "section1-data-simulation"
      },
      "source": [
        "<a name=\"data-simulation\"></a>\n",
        "### Data Simulation: Creating Sales Data\n",
        "\n",
        "We simulate 90 days of ice cream sales data. The features include:\n",
        "\n",
        "- **Temperature:** Simulated using a normal distribution (mean = 25°C, standard deviation = 3).\n",
        "- **Promotion:** A binary variable indicating whether a promotion is active (1) or not (0), with a 30% chance of promotion.\n",
        "\n",
        "Sales are computed using the formula:\n",
        "\n",
        "  sales = 300 + 12 × temperature + 60 × promotion + random noise (mean = 0, std = 20)\n",
        "\n",
        "This formula mimics a scenario where higher temperatures and promotions increase sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-simulation",
      "metadata": {
        "id": "code-simulation"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# 1. Generate dates (90 days)\n",
        "n_days = 90\n",
        "start_date = datetime(2024, 1, 1)\n",
        "dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
        "\n",
        "# 2. Generate temperature and promotion features\n",
        "temperatures = np.random.normal(loc=25, scale=3, size=n_days).round(1)\n",
        "promotions = np.random.choice([0, 1], size=n_days, p=[0.7, 0.3])\n",
        "\n",
        "# 3. Compute sales using the given formula\n",
        "sales = 300 + 12 * temperatures + 60 * promotions + np.random.normal(0, 20, size=n_days)\n",
        "\n",
        "# 4. Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'temperature': temperatures,\n",
        "    'promotion': promotions,\n",
        "    'sales': sales.round().astype(int)\n",
        "})\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\n(Data simulation successful.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section1-data-viz",
      "metadata": {
        "id": "section1-data-viz"
      },
      "source": [
        "<a name=\"data-visualization\"></a>\n",
        "### Data Visualization: Spotting Trends\n",
        "\n",
        "Visualizing the data helps us understand patterns. We will create two visualizations:\n",
        "\n",
        "1. **Scatter Plot (Temperature vs Sales):** This plot shows how temperature and promotions influence sales.\n",
        "2. **Boxplot (Promotion vs Sales):** This plot compares sales on days with and without promotions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-viz-scatter",
      "metadata": {
        "id": "code-viz-scatter"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(df['temperature'], df['sales'], c=df['promotion'], cmap='coolwarm', alpha=0.7)\n",
        "plt.title('Temperature vs Sales (Color by Promotion)')\n",
        "plt.xlabel('Temperature (°C)')\n",
        "plt.ylabel('Sales')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Promotion', rotation=270, labelpad=15)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-viz-boxplot",
      "metadata": {
        "id": "code-viz-boxplot"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "groups = df.groupby('promotion')['sales']\n",
        "labels = ['No Promo (0)', 'Promo (1)']\n",
        "data_list = [groups.get_group(g) for g in sorted(groups.groups.keys())]\n",
        "plt.boxplot(data_list, labels=labels)\n",
        "plt.title('Promotion vs Sales - Boxplot')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The boxplot shows higher sales during promotions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section2-header",
      "metadata": {
        "id": "section2-header"
      },
      "source": [
        "## 2. An Incorrect Approach\n",
        "\n",
        "**2.1 Using the Entire Dataset**\n",
        "\n",
        "A naïve approach is to use the entire dataset for both training and evaluation. Although the model may appear to perform well (a high R² score), this approach can conceal issues such as overfitting and does not reflect the model's true generalization capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-naive",
      "metadata": {
        "id": "code-naive"
      },
      "outputs": [],
      "source": [
        "print(\"=== Naïve Approach: Using the entire dataset for training and evaluation ===\")\n",
        "\n",
        "X_all = df[['temperature', 'promotion']]\n",
        "y_all = df['sales']\n",
        "\n",
        "model_all = RandomForestRegressor(random_state=42)\n",
        "model_all.fit(X_all, y_all)\n",
        "\n",
        "r2_all = model_all.score(X_all, y_all)\n",
        "print(f\"Model R^2 using the entire dataset: {r2_all:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section2-leakage",
      "metadata": {
        "id": "section2-leakage"
      },
      "source": [
        "**2.2 Demonstrating Data Leakage**\n",
        "\n",
        "Data leakage occurs when information from outside the training dataset is used to create the model. Here, a common mistake is to include the target variable as a feature. This example demonstrates how leakage can lead to unrealistically high performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-leakage",
      "metadata": {
        "id": "code-leakage"
      },
      "outputs": [],
      "source": [
        "print(\"=== Demonstrating Data Leakage ===\")\n",
        "\n",
        "X_leak = df[['temperature', 'promotion']].copy()\n",
        "X_leak['target_leak'] = df['sales']  # Incorrectly adding the target as a feature\n",
        "y_leak = df['sales']\n",
        "\n",
        "X_train_leak, X_test_leak, y_train_leak, y_test_leak = train_test_split(\n",
        "    X_leak, y_leak, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "leak_model = RandomForestRegressor(random_state=42)\n",
        "leak_model.fit(X_train_leak, y_train_leak)\n",
        "\n",
        "train_score_leak = leak_model.score(X_train_leak, y_train_leak)\n",
        "test_score_leak = leak_model.score(X_test_leak, y_test_leak)\n",
        "\n",
        "print(f\"Training R^2 with leakage: {train_score_leak:.4f}\")\n",
        "print(f\"Testing R^2 with leakage: {test_score_leak:.4f}\")\n",
        "print(\"Notice how the test score is suspiciously high—this indicates leakage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section3-header",
      "metadata": {
        "id": "section3-header"
      },
      "source": [
        "## 3. Train, Validation, and Test Datasets\n",
        "\n",
        "In more advanced settings, data is often divided into three sets:\n",
        "\n",
        "- **Training Set:** Used for model fitting.\n",
        "- **Validation Set:** Used to fine-tune the model (e.g., hyperparameter tuning).\n",
        "- **Test Set:** Used for a final unbiased evaluation of the model.\n",
        "\n",
        "Additionally, environments like Anaconda allow for easy packaging and environment management, helping maintain reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section4-header",
      "metadata": {
        "id": "section4-header"
      },
      "source": [
        "## 4. The ML Pipeline\n",
        "\n",
        "In this section, we introduce the concept of an ML pipeline and illustrate one important aspect: the difference between **fit()** and **transform()**. This is demonstrated below using the temperature feature.\n",
        "\n",
        "A comprehensive machine learning pipeline involves:\n",
        "\n",
        "1. **Data Preparation and Cleaning:** Select and clean your features.\n",
        "2. **Feature Engineering:** Create and select relevant features.\n",
        "3. **Data Splitting:** Divide data into training, validation, and test sets.\n",
        "4. **Model Selection and Training:** Experiment with and tune various models.\n",
        "5. **Evaluation:** Rigorously assess model performance.\n",
        "6. **Deployment and Monitoring:** Deploy the model and monitor its performance over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fit-vs-transform",
      "metadata": {
        "id": "fit-vs-transform"
      },
      "source": [
        "<a name=\"fit-vs-transform\"></a>\n",
        "### Detailed Illustration: fit() vs. transform()\n",
        "\n",
        "When preprocessing, **fit()** calculates necessary parameters (e.g., mean and standard deviation) from the training data, while **transform()** applies those parameters to new data. It is critical to:\n",
        "\n",
        "- Use `fit_transform()` **only** on the training data.\n",
        "- Use `transform()` on the test data.\n",
        "\n",
        "The following visualizations illustrate this difference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "visual-correct",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "visual-correct",
        "outputId": "6e52d365-1074-41cd-e433-90c6f80fa8fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dcb7a2b195f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using only the temperature feature for demonstration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_ice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_ice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Using only the temperature feature for demonstration\n",
        "X_ice = df[['temperature']]\n",
        "y_ice = df['sales']\n",
        "\n",
        "# Split the data\n",
        "X_train_ice, X_test_ice, y_train_ice, y_test_ice = train_test_split(\n",
        "    X_ice, y_ice, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Use the correct approach: fit on training data, transform test data\n",
        "scaler_correct = StandardScaler()\n",
        "X_train_ice_scaled_correct = scaler_correct.fit_transform(X_train_ice)\n",
        "X_test_ice_scaled_correct = scaler_correct.transform(X_test_ice)\n",
        "\n",
        "# Plot histograms of the raw training data, and the scaled training and test data\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(X_train_ice, bins=15, color='skyblue', edgecolor='black')\n",
        "plt.title('Raw Training Data')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(X_train_ice_scaled_correct, bins=15, color='lightgreen', edgecolor='black')\n",
        "plt.title('Scaled Training Data')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(X_test_ice_scaled_correct, bins=15, color='salmon', edgecolor='black')\n",
        "plt.title('Transformed Test Data (Correct)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Correct approach: fit() applied on training data then transform() on test data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visual-incorrect",
      "metadata": {
        "id": "visual-incorrect"
      },
      "outputs": [],
      "source": [
        "# Demonstrate the incorrect approach: using fit_transform() on the test data (which causes leakage)\n",
        "scaler_wrong = StandardScaler()\n",
        "X_test_ice_scaled_wrong = scaler_wrong.fit_transform(X_test_ice)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(X_test_ice_scaled_wrong, bins=15, color='orchid', edgecolor='black')\n",
        "plt.title('Test Data Transformed with fit_transform (Incorrect)')\n",
        "plt.xlabel('Scaled Temperature')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Incorrect approach: fit_transform() directly on test data leaks information and changes the scaling parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section5-header",
      "metadata": {
        "id": "section5-header"
      },
      "source": [
        "## 5. A Better Approach\n",
        "\n",
        "A Better Approach is divided into three parts:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part5-1",
      "metadata": {
        "id": "part5-1"
      },
      "source": [
        "### 5.1 Proper Train-Test Split and Standardization\n",
        "\n",
        "Below is an example that demonstrates the careful approach of splitting data and standardizing only the training set, then applying the same transformation to the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-careful",
      "metadata": {
        "id": "code-careful"
      },
      "outputs": [],
      "source": [
        "print(\"=== Careful Approach: Manual Data Splitting and Standardization ===\")\n",
        "\n",
        "X = df[['temperature', 'promotion']]\n",
        "y = df['sales']\n",
        "\n",
        "# 1. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Scale the training data using fit_transform()\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# 3. Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "r2_train = model.score(X_train_scaled, y_train)\n",
        "r2_test = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Training R^2: {r2_train:.4f}\")\n",
        "print(f\"Testing R^2: {r2_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part5-2",
      "metadata": {
        "id": "part5-2"
      },
      "source": [
        "### 5.2 Leveraging Pipelines to Prevent Data Leakage\n",
        "\n",
        "Using pipelines helps encapsulate preprocessing and modeling steps, ensuring a leak-free process. The following cells demonstrate building and using an ML pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-pipelines",
      "metadata": {
        "id": "code-pipelines"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "print(\"=== Using a Pipeline to Combine Preprocessing and Modeling ===\")\n",
        "\n",
        "# Split the data\n",
        "X_pipeline = df[['temperature', 'promotion']]\n",
        "y_pipeline = df['sales']\n",
        "\n",
        "X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(\n",
        "    X_pipeline, y_pipeline, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create a preprocessor that scales 'temperature' and leaves 'promotion' unchanged\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('scale_temp', StandardScaler(), ['temperature'])\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Build the pipeline: preprocessing followed by model training\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Fit the pipeline (fit_transform is applied on the training data internally)\n",
        "pipeline.fit(X_train_pipe, y_train_pipe)\n",
        "\n",
        "r2_train_pipe = pipeline.score(X_train_pipe, y_train_pipe)\n",
        "r2_test_pipe = pipeline.score(X_test_pipe, y_test_pipe)\n",
        "\n",
        "print(f\"Pipeline Training R^2: {r2_train_pipe:.4f}\")\n",
        "print(f\"Pipeline Testing R^2: {r2_test_pipe:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part5-3",
      "metadata": {
        "id": "part5-3"
      },
      "source": [
        "### 5.3 Model Persistence with pickle: Saving the Best Model\n",
        "\n",
        "After training a robust model using the pipeline, it is important to save it for future use. The following cell demonstrates how to serialize and deserialize the pipeline using `pickle`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-persistence",
      "metadata": {
        "id": "code-persistence"
      },
      "outputs": [],
      "source": [
        "print(\"=== Saving and Loading the Pipeline using pickle ===\")\n",
        "\n",
        "model_filename = \"model_pipeline.pkl\"\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "print(f\"Model has been saved to {model_filename}\")\n",
        "\n",
        "# To load the model later:\n",
        "with open(model_filename, 'rb') as f:\n",
        "    loaded_pipeline = pickle.load(f)\n",
        "print(f\"Pipeline loaded from {model_filename} and ready to use.\")\n",
        "\n",
        "# Validate the loaded model by scoring on the test data\n",
        "loaded_test_score = loaded_pipeline.score(X_test_pipe, y_test_pipe)\n",
        "print(f\"Loaded Pipeline Testing R^2: {loaded_test_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section6",
      "metadata": {
        "id": "section6"
      },
      "source": [
        "## 6. Summary and Best Practices\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "- **Prevent Data Leakage:** Never allow test data to influence model training. Always ensure that preprocessing is done separately for training and test sets.\n",
        "- **Data Splitting:** Properly divide your dataset into training, validation, and test sets for a more accurate evaluation.\n",
        "- **Use Pipelines:** Encapsulate preprocessing and modeling steps into a pipeline to maintain a consistent and leak-free process.\n",
        "- **Model Persistence:** Save your trained model using tools like pickle to ensure reproducibility and future deployment.\n",
        "- **Development Tools:** Leverage environments and tools (such as Anaconda) for easy management of packages and dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Anaconda (Optional, but Recommended)\n",
        "\n",
        "For a robust and reliable data science environment, it is recommended to install Anaconda. Anaconda simplifies package management and provides a pre-configured Python distribution with many of the required libraries.\n",
        "\n",
        "**To install Anaconda:**\n",
        "1. Visit the official [Anaconda Distribution](https://www.anaconda.com/products/distribution) page.\n",
        "2. Download the installer for your operating system (Windows, macOS, or Linux).\n",
        "3. Follow the installation instructions provided on the website.\n",
        "4. Once installed, open Anaconda Navigator and launch Jupyter Notebook to begin working."
      ],
      "metadata": {
        "id": "Y2xDsaCpVjR_"
      },
      "id": "Y2xDsaCpVjR_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}