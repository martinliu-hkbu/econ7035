{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d6ef8a6-8bc0-4e4f-adfd-6b3b78874246",
      "metadata": {},
      "source": [
        "# Lecture Notes: Preventing Data Leakage and Building Robust Machine Learning Pipelines\n",
        "\n",
        "In these notes, we cover how to simulate and visualize data, prevent data leakage with proper splits and preprocessing, leverage pipelines, and persist models using an ice cream sales forecasting example.\n",
        "\n",
        "## Table of Contents (5 Sections)\n",
        "1. Introduction & Environment Setup\n",
        "2. Data Simulation, Visualization & Extended Concepts\n",
        "3. Preventing Data Leakage\n",
        "4. Using Pipelines, a Careful Approach & Model Persistence\n",
        "5. Summary & Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b7460a-eef7-4b63-8c5f-2820432d3e55",
      "metadata": {},
      "source": [
        "## 1. Introduction & Environment Setup\n",
        "\n",
        "In this lecture, we explore how to forecast ice cream sales while avoiding data leakage. We will learn how to simulate data, properly preprocess it, build robust machine learning pipelines, and save our models for future use.\n",
        "\n",
        "Let's start by importing our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a1e5a3-1d17-41d8-9a36-9a9378cb1a98",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import pickle  # For model serialization\n",
        "\n",
        "print(\"All libraries have been successfully imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a046d60a-8ce9-4924-bb8a-863c5fa2ac43",
      "metadata": {},
      "source": [
        "## 2. Data Simulation, Visualization & Extended Concepts\n",
        "\n",
        "### Data Simulation\n",
        "\n",
        "We simulate 90 days of ice cream sales data. The features include:\n",
        "\n",
        "- **Temperature:** Simulated using a normal distribution with a mean of 25°C and a standard deviation of 3.\n",
        "- **Promotion:** A binary feature representing whether a promotion was active (1) or not (0) with a 30% chance.\n",
        "\n",
        "Sales are computed using the formula:\n",
        "\n",
        "  sales = 300 + 12 × temperature + 60 × promotion + random noise (mean = 0, std = 20)\n",
        "\n",
        "This formula mimics a scenario in which higher temperatures and promotions increase ice cream sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa9338c-112c-4588-8a5e-efe8ad3a9cfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Generate dates (90 days)\n",
        "n_days = 90\n",
        "start_date = datetime(2024, 1, 1)\n",
        "dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
        "\n",
        "# Generate temperature and promotion features\n",
        "temperatures = np.random.normal(loc=25, scale=3, size=n_days).round(1)\n",
        "promotions = np.random.choice([0, 1], size=n_days, p=[0.7, 0.3])\n",
        "\n",
        "# Compute sales using the given formula\n",
        "sales = 300 + 12 * temperatures + 60 * promotions + np.random.normal(0, 20, size=n_days)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'temperature': temperatures,\n",
        "    'promotion': promotions,\n",
        "    'sales': sales.round().astype(int)\n",
        "})\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\n(Data simulation successful.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d972542-4e6d-4d76-bb1a-8a47f275b20e",
      "metadata": {},
      "source": [
        "### Data Visualization\n",
        "\n",
        "Visualizing data helps reveal patterns and trends. We will create two plots:\n",
        "\n",
        "1. **Scatter Plot (Temperature vs Sales):** Shows how temperature (and promotions) affect sales.\n",
        "2. **Boxplot (Promotion vs Sales):** Compares sales on days with and without promotions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0f1a90-dc90-473d-92cc-58b85382a55f",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(df['temperature'], df['sales'], c=df['promotion'], cmap='coolwarm', alpha=0.7)\n",
        "plt.title('Temperature vs Sales (Color by Promotion)')\n",
        "plt.xlabel('Temperature (°C)')\n",
        "plt.ylabel('Sales')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Promotion', rotation=270, labelpad=15)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1c4a82-f91e-4f6e-87a2-41c41b6e9abb",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "groups = df.groupby('promotion')['sales']\n",
        "labels = ['No Promo (0)', 'Promo (1)']\n",
        "data_list = [groups.get_group(g) for g in sorted(groups.groups.keys())]\n",
        "plt.boxplot(data_list, labels=labels)\n",
        "plt.title('Promotion vs Sales - Boxplot')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The boxplot shows higher sales during promotions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extended-concepts",
      "metadata": {},
      "source": [
        "## Extended Concepts: Data Splitting & The Full ML Pipeline\n",
        "\n",
        "In more advanced workflows, it is common to split data into multiple sets:\n",
        "\n",
        "- **Training Set:** Used for fitting model parameters.\n",
        "- **Validation Set:** Used for hyperparameter tuning and model selection.\n",
        "- **Test Set:** Used for final unbiased evaluation.\n",
        "\n",
        "A complete machine learning pipeline encompasses:\n",
        "1. **Data Preparation and Cleaning:** Selecting and cleaning your features.\n",
        "2. **Feature Engineering:** Creating new features or transforming data.\n",
        "3. **Data Splitting:** Reserving data for training, validation, and testing.\n",
        "4. **Model Selection and Training:** Experimenting with different algorithms and tuning parameters.\n",
        "5. **Evaluation:** Assessing model performance thoroughly.\n",
        "6. **Deployment and Monitoring:** Deploying the model and tracking its performance over time.\n",
        "\n",
        "By incorporating these steps and splitting your data properly, you further reduce the risk of data leakage and improve the robustness of your machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f13871c-ea4c-45c9-81bc-8c024a943c9e",
      "metadata": {},
      "source": [
        "## 3. Preventing Data Leakage\n",
        "\n",
        "Data leakage happens when information from the test set inadvertently influences model training. A common mistake is to include the target variable as a feature.\n",
        "\n",
        "### Example: Naïve Approach\n",
        "\n",
        "A naïve approach uses the entire dataset for both training and evaluation. Although the model may achieve a high R² score, it does not reflect the model's true performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e91b5f-d3bd-496a-9d8c-9c8cb30b7e3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Naïve Approach: Using the entire dataset ===\")\n",
        "\n",
        "X_all = df[['temperature', 'promotion']]\n",
        "y_all = df['sales']\n",
        "\n",
        "model_all = RandomForestRegressor(random_state=42)\n",
        "model_all.fit(X_all, y_all)\n",
        "\n",
        "r2_all = model_all.score(X_all, y_all)\n",
        "print(f\"Model R^2 (all data): {r2_all:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f88c8e-88ea-4464-b42b-5b183b4063b2",
      "metadata": {},
      "source": [
        "### Demonstrating Data Leakage\n",
        "\n",
        "Including the target variable as part of the feature set (an accidental 'leak') can result in unrealistically high performance. In the code below, the target is mistakenly added as a feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0990244c-cb14-4e4e-bfcd-6be269afe9a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Demonstrating Data Leakage ===\")\n",
        "\n",
        "X_leak = df[['temperature', 'promotion']].copy()\n",
        "X_leak['target_leak'] = df['sales']  # Incorrectly add the target\n",
        "y_leak = df['sales']\n",
        "\n",
        "X_train_leak, X_test_leak, y_train_leak, y_test_leak = train_test_split(\n",
        "    X_leak, y_leak, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "leak_model = RandomForestRegressor(random_state=42)\n",
        "leak_model.fit(X_train_leak, y_train_leak)\n",
        "\n",
        "train_score_leak = leak_model.score(X_train_leak, y_train_leak)\n",
        "test_score_leak = leak_model.score(X_test_leak, y_test_leak)\n",
        "\n",
        "print(f\"Training R^2 (with leakage): {train_score_leak:.4f}\")\n",
        "print(f\"Testing R^2 (with leakage): {test_score_leak:.4f}\")\n",
        "print(\"Notice how the test score is suspiciously high – a sign of leakage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07b0da3-6b49-47ab-99c9-1d4fc1d2b7e0",
      "metadata": {},
      "source": [
        "## 4. Using Pipelines, a Careful Approach & Model Persistence\n",
        "\n",
        "A robust approach entails splitting data into training and test sets, applying careful preprocessing (such as standardizing only the training data), and then using pipelines to encapsulate these steps. Finally, the trained model is persisted for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6f6034-e834-4fd8-a1df-5d95ac87083c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Careful Approach: Data Splitting and Standardization ===\")\n",
        "\n",
        "X = df[['temperature', 'promotion']]\n",
        "y = df['sales']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "r2_train = model.score(X_train_scaled, y_train)\n",
        "r2_test = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Training R^2: {r2_train:.4f}\")\n",
        "print(f\"Testing R^2: {r2_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8820175-1a6b-4ff3-a8c8-3e1fe77a66a8",
      "metadata": {},
      "source": [
        "### Using Pipelines to Prevent Data Leakage\n",
        "\n",
        "Pipelines allow you to encapsulate preprocessing and modeling steps in one object. This ensures that the correct operations (like fitting a scaler only on training data) are applied in a fixed order.\n",
        "\n",
        "Below, we create a pipeline to scale the 'temperature' feature and train a Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab427dd0-2e3e-45e3-8346-2fa2d0958ac8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "print(\"=== Pipeline Construction and Modeling ===\")\n",
        "\n",
        "# Prepare data for the pipeline\n",
        "X_pipeline = df[['temperature', 'promotion']]\n",
        "y_pipeline = df['sales']\n",
        "\n",
        "X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(\n",
        "    X_pipeline, y_pipeline, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define a preprocessor to scale 'temperature'\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('scale_temp', StandardScaler(), ['temperature'])\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Build the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train_pipe, y_train_pipe)\n",
        "\n",
        "r2_train_pipe = pipeline.score(X_train_pipe, y_train_pipe)\n",
        "r2_test_pipe = pipeline.score(X_test_pipe, y_test_pipe)\n",
        "\n",
        "print(f\"Pipeline Training R^2: {r2_train_pipe:.4f}\")\n",
        "print(f\"Pipeline Testing R^2: {r2_test_pipe:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7628d33e-7eea-45c4-ae6d-2f3b8549babc",
      "metadata": {},
      "source": [
        "### Model Persistence\n",
        "\n",
        "Once you have trained a robust model, it is important to save it so that it can be used later without retraining. We use the `pickle` module to serialize and deserialize our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd3d3e6-1a11-44c5-abbc-6b75d201b038",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Saving and Loading the Pipeline using pickle ===\")\n",
        "\n",
        "model_filename = \"model_pipeline.pkl\"\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "print(f\"Model has been saved to {model_filename}\")\n",
        "\n",
        "# Later, load the saved model\n",
        "with open(model_filename, 'rb') as f:\n",
        "    loaded_pipeline = pickle.load(f)\n",
        "print(f\"Loaded pipeline from {model_filename} and ready to use.\")\n",
        "\n",
        "# Validate loaded pipeline\n",
        "loaded_test_score = loaded_pipeline.score(X_test_pipe, y_test_pipe)\n",
        "print(f\"Loaded Pipeline Testing R^2: {loaded_test_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c88b86e-b348-4a5f-9c2e-9b8f3e9c762f",
      "metadata": {},
      "source": [
        "## 5. Summary & Best Practices\n",
        "\n",
        "**Key Recommendations:**\n",
        "\n",
        "- **Prevent Data Leakage:** Always ensure that preprocessing on test data is performed separately (or via pipelines) so that test data does not influence model training.\n",
        "- **Proper Data Splitting:** Divide your data into training (and optionally validation) and test sets to evaluate how the model will generalize.\n",
        "- **Leverage Pipelines:** Use pipelines to enforce a consistent, leak-free workflow from preprocessing to modeling.\n",
        "- **Save Your Work:** Persist your developed models using tools like pickle for reproducibility and future deployment.\n",
        "\n",
        "### Installing Anaconda (Optional, but Recommended)\n",
        "\n",
        "For a robust and reliable data science environment, it is recommended to install Anaconda. Anaconda simplifies package management and provides a pre-configured Python distribution with many of the required libraries.\n",
        "\n",
        "**To install Anaconda:**\n",
        "1. Visit the official [Anaconda Distribution](https://www.anaconda.com/products/distribution) page.\n",
        "2. Download the installer for your operating system (Windows, macOS, or Linux).\n",
        "3. Follow the installation instructions provided on the website.\n",
        "4. Once installed, open Anaconda Navigator and launch Jupyter Notebook to begin working.\n",
        "\n",
        "By following these best practices, you can build robust machine learning models that generalize well and are production-ready.\n",
        "\n",
        "Happy modeling!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
