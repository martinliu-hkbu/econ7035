{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ae55d",
   "metadata": {},
   "source": [
    "## The Data: Boston house prices dataset\n",
    "\n",
    "The dataset contain 506 towns' median house price and each town's information.  There are 13 features and 1 target (in order):\n",
    "- **CRIM**     per capita crime rate by town\n",
    "- **ZN**       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- **INDUS**    proportion of non-retail business acres per town\n",
    "- **CHAS**     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- **NOX**      nitric oxides concentration (parts per 10 million)\n",
    "- **RM**       average number of rooms per dwelling\n",
    "- **AGE**      proportion of owner-occupied units built prior to 1940\n",
    "- **DIS**      weighted distances to five Boston employment centres\n",
    "- **RAD**      index of accessibility to radial highways\n",
    "- **TAX**      full-value property-tax rate per 10,000\n",
    "- **PTRATIO**  pupil-teacher ratio by town\n",
    "- **B**        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
    "- **LSTAT**    % lower status of the population\n",
    "- **MEDV**     Median value of owner-occupied homes in 1000s (target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb3516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('boston_house_prices.csv')    \n",
    "\n",
    "df.describe()                # Need scaling? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39fb8f",
   "metadata": {},
   "source": [
    "Split the data into Train and Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13758c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'MEDV')\n",
    "\n",
    "y = df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)    \n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b61704",
   "metadata": {},
   "source": [
    "# 1. Linear Regression with a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36716515",
   "metadata": {},
   "source": [
    "With the help of ``Pipeline``, let's build a ``Linear Regression`` with ``MinMaxScaler`` and  ``PolynomialFeatures`` applied for data preprocessing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build the pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "lr_pipe = Pipeline([(\"scaler\", MinMaxScaler()), \n",
    "                    (\"poly\",PolynomialFeatures(degree = 2, include_bias = False)),  # no need get 1 for intercept \n",
    "                    (\"model\", LinearRegression())])    \n",
    "\n",
    "lr_pipe.named_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train the pipeline and check performance\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)   \n",
    "\n",
    "print(\"lr_pipe Train R2: {:.2f}\".format(lr_pipe.score(X_train, y_train)))   \n",
    "print(\"lr_pipe Test R2: {:.2f}\".format(lr_pipe.score(X_test, y_test)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e20a26",
   "metadata": {},
   "source": [
    "Note that as ``lr_pipe`` is a ``pipeline``, we need to call each step by their step names and apply the associated methods for result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7401819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get features and coefs \n",
    "\n",
    "poly = lr_pipe.named_steps['poly']  # Get the polynomial transformer\n",
    "\n",
    "lr = lr_pipe.named_steps['model']         # Get the model  \n",
    "\n",
    "pd.DataFrame({'features': poly.get_feature_names_out(), 'coefs':lr.coef_})   \n",
    "\n",
    "# Get 104 features in total (orginally 13)\n",
    "# New feature names (x0,x1..x12) were auto-generated corresponding to their original column index. \n",
    "# To check original feature names: X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15120fb0",
   "metadata": {},
   "source": [
    "**The linear regression with too many polynomrial features overfits**\n",
    "\n",
    "As indicated by the huge gap between the train and test R2, **the linear regression model might have overfitted**, so we need to control model complexity a bit by regularizing the coefficient magnitude.  \n",
    "\n",
    "- [Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) regularize model complexity by penalizing the sum of squares (L2-norms) of coefficients, which will reduce feature importance (i.e., coef) to be as ``close to 0``.\n",
    "\n",
    "- [Lasso Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) control model complexity by penalizing the sum of absolute values (L1-norms) of coefficients. As it reduce feature importance (i.e. coef) to be ``exactly 0``, it often works for automatical feature selection.\n",
    "\n",
    "For both ``Ridge`` and ``Lasso``, the strength of penalty is controlled by the parameter ``alpha`` (default = 1). A larger ``alpha`` value means larger panelty in the objective function (which we aims to minimize) and therefore stronger regularization on the coefficients, leading to a simpler model (with smaller coefs).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6b24c",
   "metadata": {},
   "source": [
    "# 2. Ridge Regression\n",
    "\n",
    "## 2.1 ``Ridge`` Models with Different Regularization Strength\n",
    "\n",
    "With the help of pipelines, build three ``Ridge`` models with different ``alpha`` values (i.e., ``0.1``, ``1``, and ``10``).\n",
    "\n",
    "- use ``MinMaxScaler`` and ``PolynomialFeatures`` with ``degree`` = 2. (same as ``lr_pipe``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7045ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: build the pipelines\n",
    "\n",
    "ridge1_pipe = Pipeline([(\"scaler\", MinMaxScaler()), \n",
    "                        (\"poly\",PolynomialFeatures(degree = 2, include_bias = False)),   \n",
    "                        (\"model\", Ridge(alpha = 0.1))])\n",
    "\n",
    "ridge2_pipe = Pipeline([(\"scaler\", MinMaxScaler()), \n",
    "                        (\"poly\",PolynomialFeatures(degree = 2, include_bias = False)),   \n",
    "                        (\"model\", Ridge())])      # Default alpha = 1\n",
    "\n",
    "ridge3_pipe = Pipeline([(\"scaler\", MinMaxScaler()), \n",
    "                        (\"poly\",PolynomialFeatures(degree = 2, include_bias = False)),   \n",
    "                        (\"model\", Ridge(alpha = 10))])\n",
    "\n",
    "display(ridge1_pipe.named_steps, ridge2_pipe.named_steps, ridge3_pipe.named_steps)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train the pipeline and check performance\n",
    "\n",
    "ridge1_pipe.fit(X_train, y_train)\n",
    "ridge2_pipe.fit(X_train, y_train)\n",
    "ridge3_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"ridge1(alpha = 0.1) Train & Test R2: {:.2f} & {:.2f}\".format(ridge1_pipe.score(X_train, y_train),\n",
    "                                                                    ridge1_pipe.score(X_test, y_test)))\n",
    "\n",
    "print(\"ridge2(alpha = 1) Train & Test R2: {:.2f} & {:.2f}\".format(ridge2_pipe.score(X_train, y_train),\n",
    "                                                                  ridge2_pipe.score(X_test, y_test)))\n",
    "\n",
    "print(\"ridge3(alpha = 10) Train & Test R2: {:.2f} & {:.2f}\".format(ridge3_pipe.score(X_train, y_train), \n",
    "                                                                   ridge3_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19eea5d",
   "metadata": {},
   "source": [
    "Now you can see, the best ridge model is ``ridge1_pipe`` (with ``alpha`` = 0.1), which return a worse train R2 (0.93) but better test R2 (0.75) compared with ``lr_pipe`` (0.95 and 0.62 respectively). \n",
    "\n",
    "-  ``ridge2_pipe`` (``alpha`` = 1) is a simpler model, worse train and test R2 compared with ``ridge1_pipe`` (starting to underfit).\n",
    "-  ``ridge3_pipe`` (``alpha`` = 10) is too simple, worse train and test R2 (more underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad247a4",
   "metadata": {},
   "source": [
    "**Any change in coefficients?**\n",
    "\n",
    "Let's get coefficients for all three models, compared their ``sum of squared coefficients`` and visualize the magnitude of coefficients. \n",
    "\n",
    "- Larger ``alpha`` means smaller ``sum of squared coefficients``.  \n",
    "- Linear regression can be seen as regularized regression with ``alpha`` = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Get features and coefs  \n",
    "\n",
    "poly1 = ridge1_pipe.named_steps['poly']  # get the polynomial transformer  \n",
    "\n",
    "ridge1 = ridge1_pipe.named_steps['model']      # get ridge1 model  \n",
    "ridge2 = ridge2_pipe.named_steps['model']\n",
    "ridge3 = ridge3_pipe.named_steps['model']\n",
    "\n",
    "# Put all coefs in a dataframe for better visualization, which feature is most important? use .max()\n",
    "pd.DataFrame({'features': poly1.get_feature_names_out(),    # same features for three models\n",
    "              'ridge1_coefs':ridge1.coef_,\n",
    "              'ridge2_coefs':ridge2.coef_,\n",
    "              'ridge3_coefs':ridge2.coef_})      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum of squared coefficients: bigger alpha, smaller sum of squared coefs\n",
    "\n",
    "print(\"lr sum of squared coefs: {:.2f}\".format(sum((lr.coef_)**2)))  \n",
    "print(\"ridge1 sum of squared coefs: {:.2f}\".format(sum((ridge1.coef_)**2))) \n",
    "print(\"ridge2 sum of squared coefs: {:.2f}\".format(sum((ridge2.coef_)**2))) \n",
    "print(\"ridge3 sum of squared coefs: {:.2f}\".format(sum((ridge3.coef_)**2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661374c1",
   "metadata": {},
   "source": [
    "**Visualize the Coefficients**\n",
    "\n",
    "Now let's visualize the coefficients (i.e. slopes) for comparison, with linear regression's coefficients included for comparison.\n",
    "- When ``alpha`` = 10, most coefs close (but not equal) to 0 (blue squares).\n",
    "- Reducing ``alpha`` to 1,  magnitude of coefs increases (red crosses). \n",
    "- When ``alpha`` = 0.1 (our best model so far), more coefs with large magnitudes (green triangles).\n",
    "- The coefs of Linear regression are most spread out (orange circles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d539bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Note y value = coefficients, x values = their index in the array (auto).\n",
    "plt.plot(lr.coef_, 'o', color = 'orange', label=\"LinearRegression\")            # circle\n",
    "plt.plot(ridge1.coef_, '^', color = 'lightgreen',  label=\"Ridge alpha=0.1\")    # triangle\n",
    "plt.plot(ridge2.coef_, '+', color = 'red',label=\"Ridge alpha=1\")               # cross\n",
    "plt.plot(ridge3.coef_, 's', color = 'blue', label=\"Ridge alpha=10\")            # square\n",
    "\n",
    "# Visualize a reference line: y = 0\n",
    "xlims = plt.xlim()                                     # Get the x limits of the current plot\n",
    "plt.hlines(0, xlims[0], xlims[1], color = 'grey')      \n",
    "plt.ylim(-25, 25)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Coefficient index\")               # Order of each coef in the array\n",
    "plt.ylabel(\"Coefficient magnitude\")    \n",
    "plt.title('Comparing Coefficient Magnitudes for Ridge Regressions(LR as Base)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62993f9",
   "metadata": {},
   "source": [
    "## 2.2 ``GridSearchCV`` with a Ridge Pipeline\n",
    "\n",
    "Then what is the best ``degree`` value for ``PolynomialFeatures()`` and the best ``alpha`` value in ``Ridge()``? We might build a pipeline for that.  \n",
    "\n",
    "- You can use any of the above pipelines as the estimator in ``GridSearchCV`` as the param values set above will be ignored in the grid search process. \n",
    "- Pay attention to how we set the param names in the dictionary keys: step names and param names are connected with ``__`` (double underscore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea3fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "params1 = {'scaler':[MinMaxScaler(),StandardScaler()],      # Compare 2 scalers\n",
    "           'poly__degree': [2,3,4,5],                 # Compare 4 degree values  \n",
    "           'model__alpha': [0.0001,0.001,0.1,1,2]}          # Compare 5 alpha values\n",
    "\n",
    "grid1 = GridSearchCV(estimator = ridge1_pipe, param_grid = params1, cv=5)    # Use an ridge1_pipe\n",
    "\n",
    "grid1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params: {}\".format(grid1.best_params_))    \n",
    "print(\"Best cross-validation R2: {}\".format(grid1.best_score_))   # Best mean cv score on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c701c80",
   "metadata": {},
   "source": [
    "Check the performance of the best model, which is refit on entire train set with the best prams chosen in cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = grid1.score(X_test, y_test)    # Apply the best model (a pipe) on test data directly! \n",
    "\n",
    "print(\"Test score of the best model: {:.2f}\".format(test_score))   # Even better than best ridge1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67caeb7",
   "metadata": {},
   "source": [
    "Check the coefficients of the best model.\n",
    "\n",
    "- Note the best model is a pipeline, not a regression model.  You need to call each step by their names and apply the associated methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7927988",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = grid1.best_estimator_                 # This is a pipeline\n",
    "\n",
    "best_poly = best_pipe.named_steps['poly']   # Get best poly transformer\n",
    "\n",
    "best_model = best_pipe.named_steps['model']       # Get best model\n",
    "\n",
    "print(\"best_pipe Sum of squared coefs: \", sum((best_model.coef_)**2))  # Get the sum of  coefs\n",
    "\n",
    "pd.DataFrame({'grid1_features': best_poly.get_feature_names_out(), 'coefs':best_model.coef_})   # 559 features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8818d",
   "metadata": {},
   "source": [
    "# 3. Lasso Regression\n",
    "\n",
    "## 3.1  ``Lasso`` Models with Different Regularization Strength\n",
    "\n",
    "For ``Lasso`` regression, let's build 3 pipelines with different ``alpha`` values(``1``,`` 0.01``, ``0.0001``).\n",
    "\n",
    "- Same as above, let's use ``MinMaxScaler`` and ``PolynomialFeatures`` with ``degree`` = 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93796a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# degree = 2, alpha = 1 (bad performance on both train or test - underfitting)\n",
    "\n",
    "lasso1_pipe = Pipeline([(\"scaler\", MinMaxScaler()), \n",
    "                        (\"poly\",PolynomialFeatures(degree = 2, include_bias = False)), \n",
    "                        (\"model\", Lasso())])     # default alpha = 1\n",
    "build the pipelines\n",
    "lasso1_pipe.fit(X_train, y_train)   \n",
    "\n",
    "print(\"lasso1 (alpha = 1) Train R2: {:.2f}\".format(lasso1_pipe.score(X_train, y_train)))   \n",
    "print(\"lasso1 (alpha = 1) Test R2: {:.2f}\".format(lasso1_pipe.score(X_test, y_test)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfce5f",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 1: Your Codes Here***</font>  \n",
    "\n",
    "``lasso1_pipe`` is obviously underfitting, so we need to tune down the regularization strength a bit.   Please build ``lasso2_pipe`` and ``lasso3_pipe`` with ``alpha`` = ``0.01`` and ``0.0001`` respectively. For ``scaler`` and ``poly`` steps, use the same param as in ``lasso1_pipe``.\n",
    "\n",
    "- Check their performance on train and test as well.\n",
    "- Among all three lasso models, which one is the best one? \n",
    "- If we compare the best lasso model with the best ridge (i.e., ``ridge1_pipe``), which one is better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafba23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# degree = 2, alpha = 0.01 (tune down regularization a bit, better result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea7cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# degree = 2, alpha = 0.0001 (start to overfit as test R2 drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1cc08",
   "metadata": {},
   "source": [
    "**Any change in coefficients?**\n",
    "\n",
    "Let's get coefficients for all three models, compared their ``sum of absolute coefficients`` and visualize the magnitude of coefficients. \n",
    "\n",
    "- The larger ``alpha`` (stronger regulation) is, the smaller the ``sum of absolute cofficients`` is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso1_coefs = lasso1_pipe.named_steps['model'].coef_\n",
    "lasso2_coefs = lasso2_pipe.named_steps['model'].coef_\n",
    "lasso3_coefs = lasso3_pipe.named_steps['model'].coef_\n",
    "\n",
    "print(\"lasso1 (alpha = 1) sum of absolute coefficients: {:.2f} \".format(sum(abs(lasso1_coefs))))\n",
    "print(\"lasso2 (alpha = 0.01) sum of absolute coefficients:{:.2f} \".format(sum(abs(lasso2_coefs)))) \n",
    "print(\"lasso3 (alpha = 0.0001) sum of absolute coefficients: {:.2f}\".format(sum(abs(lasso3_coefs))))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb1740",
   "metadata": {},
   "source": [
    "- How many coefficients are useful (not equal to 0) for prediction in each model? \n",
    "- The larger ``alpha`` (stronger regulation) is, the more coefficients == 0, which means less feature will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb468e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"lasso1 (alpha = 1) No. of features used:\", sum(lasso1_coefs != 0))\n",
    "print(\"lasso2 (alpha = 0.01) No. of features used:\", sum(lasso2_coefs != 0))\n",
    "print(\"lasso3 (alpha = 0.0001) No. of features used:\", sum(lasso3_coefs != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94407364",
   "metadata": {},
   "source": [
    "**Visualize the Coefficients**\n",
    "\n",
    "- Include the best ridge model ``ridge1`` for reference (the orange circles, most coef non-zero).\n",
    "-  When ``alpha`` = 1, not only most coeffs are zero, the 3 non-zero coefs are very small (blue squares).\n",
    "- ``alpha`` = 0.01, more coef become non-zero, still lots are zero (red crosses) .\n",
    "- ``alpha`` = 0.0001, less regularization, most coef have larger magnititude (green trianges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19de31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(ridge1.coef_, 'o', color = 'orange',  label=\"Ridge alpha = 0.1\")          # circles\n",
    "plt.plot(lasso1_coefs, 's', color = 'blue', label=\"Lasso1 alpha = 1\")              # squares\n",
    "plt.plot(lasso2_coefs, '+', color = 'red',  label=\"Lasso2 alpha = 0.01\")           # crosses\n",
    "plt.plot(lasso3_coefs, '^', color = 'lightgreen',label=\"Lasso3 alpha = 0.0001\")    # triangle\n",
    "\n",
    "# Visualize a reference line: y = 0\n",
    "xlims = plt.xlim()                                     \n",
    "plt.hlines(0, xlims[0], xlims[1],color = 'grey')      # reference line: y = 0\n",
    "plt.ylim(-25, 25)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Coefficient index\")                       \n",
    "plt.ylabel(\"Coefficient magnitude\")  \n",
    "plt.title('Comparing Coefficient Magnitudes for Lasso Regressions (Ridge as Base)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58310c",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 ``GridSearchCV`` with a Lasso Pipeline \n",
    "\n",
    "Compare ``MinMaxScaler`` and ``StandardScaler``;  compare four values ``2,3,4,5`` for ``degree`` in ``PolynomialFeatures`` and four values ``0.0001, 0.0005, 0.001, 0.1`` for ``alpha`` in the ``Lasso`` model. (As we know ``alpha`` = 0.0001 causes overfitting already.)\n",
    "\n",
    "*Hints: You may use any of the pipeline built in section 3.1 as the parameter set above will be ignored in ``GridSearchCV``.*\n",
    "\n",
    "- Return the best params for each step and the cross-validation performance on validation fold.\n",
    "- Check the generalization performance of the best model (refit on the train set) on the test set.\n",
    "- Check how many coefficients returned by the best model are NOT zero? \n",
    "\n",
    "<font color=red>***Exercise 2: Your Codes Here***</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d09a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfec0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test R2 of best model, similiar to the best ridge via Gridsearch (test R2: 0.78)\n",
    "\n",
    "grid2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = grid2.best_estimator_      \n",
    "\n",
    "sum(best_pipe.named_steps['model'].coef_ != 0)   # a lot! as alpha = 0.001 and degree = 3\n",
    "\n",
    "#len(best_pipe.named_steps['model'].coef_)       # Total number of coef is 559  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
